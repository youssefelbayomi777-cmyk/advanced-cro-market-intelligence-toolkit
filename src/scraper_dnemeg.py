#!/usr/bin/env python3
"""
DNM.EG Website Scraper
ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ‚Ø¹ dnmeg.com Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Python Ùˆ BeautifulSoup
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin, urlparse

class DNMScraper:
    def __init__(self):
        self.base_url = "https://dnmeg.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def get_page(self, url):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø©"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            return None
    
    def extract_homepage_data(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        data = {
            'title': soup.find('title').text.strip() if soup.find('title') else '',
            'description': soup.find('meta', {'name': 'description'}).get('content', '') if soup.find('meta', {'name': 'description'}) else '',
            'products': [],
            'categories': [],
            'trust_signals': [],
            'navigation': []
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        product_elements = soup.find_all('div', class_='product-item')
        for product in product_elements:
            product_data = {
                'name': product.find('h2').text.strip() if product.find('h2') else '',
                'price': product.find('span', class_='price').text.strip() if product.find('span', class_='price') else '',
                'url': product.find('a')['href'] if product.find('a') else ''
            }
            data['products'].append(product_data)
        
        return data
    
    def extract_category_data(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª ØµÙØ­Ø§Øª Ø§Ù„ÙØ¦Ø§Øª"""
        data = {
            'category_name': soup.find('h1').text.strip() if soup.find('h1') else '',
            'products': [],
            'filters': [],
            'sorting': []
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª ÙÙŠ Ø§Ù„ÙØ¦Ø©
        product_elements = soup.find_all('div', class_='product-item')
        for product in product_elements:
            product_data = {
                'name': product.find('h2').text.strip() if product.find('h2') else '',
                'price': product.find('span', class_='price').text.strip() if product.find('span', class_='price') else '',
                'url': product.find('a')['href'] if product.find('a') else ''
            }
            data['products'].append(product_data)
        
        return data
    
    def scrape_site(self):
        """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù…ÙˆÙ‚Ø¹ dnmeg.com...")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
        homepage = self.get_page(self.base_url)
        if homepage:
            homepage_data = self.extract_homepage_data(homepage)
            print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(homepage_data['products'])} Ù…Ù†ØªØ¬ ÙÙŠ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")
        
        # ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        print("ğŸ“¦ ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª...")
        products_data = []
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        if homepage:
            product_links = homepage.find_all('a', href=True)
            for link in product_links:
                if '/products/' in link['href']:
                    product_url = urljoin(self.base_url, link['href'])
                    print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù†ØªØ¬: {product_url}")
                    
                    product_page = self.get_page(product_url)
                    if product_page:
                        product_data = self.extract_product_data(product_page)
                        products_data.append(product_data)
                    
                    time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ù„Ù…Ù†Ø¹ Ø§Ù„Ø­Ø¸Ø±
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        final_data = {
            'scrape_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'homepage': homepage_data if homepage else {},
            'products': products_data,
            'total_products': len(products_data),
            'analysis_summary': {
                'homepage_products': len(homepage_data['products']) if homepage else 0,
                'total_products_found': len(products_data)
            }
        }
        
        # Ø­ÙØ¸ ÙÙŠ Ù…Ù„Ù JSON
        with open('dnmeg_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {len(products_data)} Ù…Ù†ØªØ¬ Ø¨Ù†Ø¬Ø§Ø­!")
        print("ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ dnmemeg_analysis.json")
        
        return final_data
    
    def extract_product_data(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬ Ø§Ù„Ù…ÙØµÙ„Ø©"""
        data = {
            'name': '',
            'price': '',
            'description': '',
            'images': [],
            'specifications': {},
            'availability': '',
            'reviews': []
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬
        title_element = soup.find('h1') or soup.find('title')
        if title_element:
            data['name'] = title_element.text.strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¹Ø±
        price_element = soup.find('span', class_='price') or soup.find('div', class_='price')
        if price_element:
            data['price'] = price_element.text.strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ
        desc_element = soup.find('div', class_='description') or soup.find('meta', {'name': 'description'})
        if desc_element:
            if desc_element.name == 'meta':
                data['description'] = desc_element.get('content', '')
            else:
                data['description'] = desc_element.text.strip()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±
        img_elements = soup.find_all('img')
        for img in img_elements:
            if img.get('src'):
                data['images'].append(img['src'])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªÙˆÙØ±
        availability_element = soup.find('span', class_='availability') or soup.find('div', class_='stock')
        if availability_element:
            data['availability'] = availability_element.text.strip()
        
        return data

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    scraper = DNMScraper()
    results = scraper.scrape_site()
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
    print("\n" + "="*50)
    print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
    print(f"ğŸ“¦ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª: {results['total_products']}")
    print(f"ğŸ  Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {results['analysis_summary']['homepage_products']}")
    print(f"ğŸ“ˆ ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„: {results['scrape_time']}")
    print("="*50)

if __name__ == "__main__":
    main()
