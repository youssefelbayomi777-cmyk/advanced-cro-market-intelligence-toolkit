#!/usr/bin/env python3
"""
DNM.EG Performance Analyzer
ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ ØªÙ‚Ù†ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù…ÙˆÙ‚Ø¹ dnmeg.com
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin, urlparse
import re

class PerformanceAnalyzer:
    def __init__(self):
        self.base_url = "https://dnmeg.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.results = {
            'page_load_times': {},
            'image_analysis': {},
            'mobile_performance': {},
            'seo_analysis': {},
            'technical_issues': [],
            'recommendations': []
        }
        
    def measure_page_load_time(self, url):
        """Ù‚ÙŠØ§Ø³ ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©"""
        try:
            start_time = time.time()
            response = self.session.get(url, timeout=10)
            load_time = time.time() - start_time
            
            # ØªØ­Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙØ­Ø©
            page_size = len(response.content) / 1024  # Ø¨Ø§Ù„ÙƒÙŠÙ„ÙˆØ¨Ø§ÙŠØª
            
            return {
                'url': url,
                'load_time': round(load_time, 3),
                'page_size_kb': round(page_size, 2),
                'status_code': response.status_code,
                'response_headers': dict(response.headers)
            }
        except Exception as e:
            return {
                'url': url,
                'error': str(e),
                'load_time': 999,
                'page_size_kb': 0,
                'status_code': 0
            }
    
    def analyze_images(self, soup, page_url):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§"""
        images = soup.find_all('img')
        image_analysis = {
            'total_images': len(images),
            'optimized_images': 0,
            'large_images': 0,
            'missing_alt': 0,
            'external_images': 0,
            'image_details': []
        }
        
        for img in images:
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
            image_info = {
                'src': src,
                'alt': alt,
                'has_alt': bool(alt),
                'is_external': not src.startswith('//dnmeg.com') and not src.startswith('/cdn/'),
                'size_estimate': 'unknown'
            }
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© (ØªÙ‚Ø¯ÙŠØ±ÙŠ)
            if 'width=' in src:
                width = re.search(r'width=(\d+)', src)
                if width:
                    width_val = int(width.group(1))
                    if width_val > 1500:
                        image_analysis['large_images'] += 1
                        image_info['is_large'] = True
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ­Ø³ÙŠÙ†
            if any(optimized in src.lower() for optimized in ['webp', 'optimized', 'compressed']):
                image_analysis['optimized_images'] += 1
                image_info['is_optimized'] = True
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† ALT text
            if not alt:
                image_analysis['missing_alt'] += 1
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµÙˆØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
            if image_info['is_external']:
                image_analysis['external_images'] += 1
            
            image_analysis['image_details'].append(image_info)
        
        return image_analysis
    
    def test_mobile_performance(self, url):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¬ÙˆØ§Ù„ (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© Ø·Ù„Ø¨ Ø§Ù„Ø¬ÙˆØ§Ù„
            mobile_headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
            }
            
            mobile_session = requests.Session()
            mobile_session.headers.update(mobile_headers)
            
            start_time = time.time()
            response = mobile_session.get(url, timeout=10)
            mobile_load_time = time.time() - start_time
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø¯Ù‰ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¬ÙˆØ§Ù„
            mobile_analysis = {
                'mobile_load_time': round(mobile_load_time, 3),
                'viewport_meta': bool(soup.find('meta', {'name': 'viewport'})),
                'responsive_images': len(soup.find_all('img', {'srcset': True})),
                'mobile_navigation': bool(soup.find('nav', class_='mobile-menu')),
                'touch_friendly': self.check_touch_friendly(soup),
                'font_sizes': self.analyze_font_sizes(soup)
            }
            
            return mobile_analysis
            
        except Exception as e:
            return {
                'error': str(e),
                'mobile_load_time': 999
            }
    
    def check_touch_friendly(self, soup):
        """ÙØ­Øµ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù„Ù…Ø³"""
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ù„Ù…Ø³
        buttons = soup.find_all(['button', 'a'], class_=re.compile(r'btn|button|touch'))
        touch_friendly = len(buttons) > 0
        
        return touch_friendly
    
    def analyze_font_sizes(self, soup):
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø®Ø·ÙˆØ·"""
        font_sizes = []
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        text_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'span'])
        
        for element in text_elements:
            style = element.get('style', '')
            if 'font-size' in style:
                font_size = re.search(r'font-size:\s*(\d+)px', style)
                if font_size:
                    font_sizes.append(int(font_size.group(1)))
        
        return {
            'min_font_size': min(font_sizes) if font_sizes else 0,
            'max_font_size': max(font_sizes) if font_sizes else 0,
            'average_font_size': sum(font_sizes) / len(font_sizes) if font_sizes else 0
        }
    
    def analyze_seo(self, soup, page_url):
        """ØªØ­Ù„ÙŠÙ„ SEO"""
        seo_analysis = {
            'title': {
                'exists': bool(soup.find('title')),
                'content': soup.find('title').text.strip() if soup.find('title') else '',
                'length': len(soup.find('title').text.strip()) if soup.find('title') else 0,
                'optimal': False
            },
            'meta_description': {
                'exists': bool(soup.find('meta', {'name': 'description'})),
                'content': soup.find('meta', {'name': 'description'}).get('content', '') if soup.find('meta', {'name': 'description'}) else '',
                'length': len(soup.find('meta', {'name': 'description'}).get('content', '')) if soup.find('meta', {'name': 'description'}) else 0,
                'optimal': False
            },
            'headings': {
                'h1_count': len(soup.find_all('h1')),
                'h2_count': len(soup.find_all('h2')),
                'h3_count': len(soup.find_all('h3')),
                'structure_ok': False
            },
            'images_alt': {
                'total_images': len(soup.find_all('img')),
                'with_alt': len([img for img in soup.find_all('img') if img.get('alt')]),
                'percentage': 0
            },
            'internal_links': {
                'total': len(soup.find_all('a', href=True)),
                'internal': 0,
                'external': 0
            }
        }
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø£Ù…Ø«Ù„
        seo_analysis['title']['optimal'] = 30 <= seo_analysis['title']['length'] <= 60
        seo_analysis['meta_description']['optimal'] = 120 <= seo_analysis['meta_description']['length'] <= 160
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        seo_analysis['headings']['structure_ok'] = (
            seo_analysis['headings']['h1_count'] == 1 and
            seo_analysis['headings']['h2_count'] > 0
        )
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© ALT text
        if seo_analysis['images_alt']['total_images'] > 0:
            seo_analysis['images_alt']['percentage'] = round(
                (seo_analysis['images_alt']['with_alt'] / seo_analysis['images_alt']['total_images']) * 100, 2
            )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('http') and not href.startswith(self.base_url):
                seo_analysis['internal_links']['external'] += 1
            else:
                seo_analysis['internal_links']['internal'] += 1
        
        return seo_analysis
    
    def identify_technical_issues(self, analysis_results):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©"""
        issues = []
        
        # Ù…Ø´Ø§ÙƒÙ„ ÙˆÙ‚Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
        for page, data in analysis_results['page_load_times'].items():
            if data.get('load_time', 0) > 3:
                issues.append({
                    'type': 'performance',
                    'severity': 'high',
                    'page': page,
                    'issue': f'Ø¨Ø·Ø¡ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {data["load_time"]} Ø«Ø§Ù†ÙŠØ©',
                    'recommendation': 'ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„ØµÙØ­Ø©'
                })
        
        # Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØµÙˆØ±
        img_analysis = analysis_results.get('image_analysis', {})
        if img_analysis.get('missing_alt', 0) > 0:
            issues.append({
                'type': 'seo',
                'severity': 'medium',
                'issue': f'ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ALT text: {img_analysis["missing_alt"]}',
                'recommendation': 'Ø¥Ø¶Ø§ÙØ© ÙˆØµÙ Ù„Ù„ØµÙˆØ± Ù„ØªØ­Ø³ÙŠÙ† SEO ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„'
            })
        
        if img_analysis.get('large_images', 0) > 0:
            issues.append({
                'type': 'performance',
                'severity': 'medium',
                'issue': f'ØµÙˆØ± ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ø­Ø¬Ù…: {img_analysis["large_images"]}',
                'recommendation': 'Ø¶ØºØ· Ø§Ù„ØµÙˆØ± ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ù„Ù„ÙˆÙŠØ¨'
            })
        
        # Ù…Ø´Ø§ÙƒÙ„ SEO
        seo_analysis = analysis_results.get('seo_analysis', {})
        if not seo_analysis.get('title', {}).get('optimal', False):
            issues.append({
                'type': 'seo',
                'severity': 'high',
                'issue': 'Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…Ø­Ø³Ù†',
                'recommendation': 'ØªØ­Ø³ÙŠÙ† Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø© Ù„ÙŠÙƒÙˆÙ† Ø¨ÙŠÙ† 30-60 Ø­Ø±Ù'
            })
        
        if not seo_analysis.get('meta_description', {}).get('optimal', False):
            issues.append({
                'type': 'seo',
                'severity': 'high',
                'issue': 'ÙˆØµÙ Ø§Ù„ØµÙØ­Ø© ØºÙŠØ± Ù…Ø­Ø³Ù†',
                'recommendation': 'ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙØ­Ø© Ù„ÙŠÙƒÙˆÙ† Ø¨ÙŠÙ† 120-160 Ø­Ø±Ù'
            })
        
        # Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬ÙˆØ§Ù„
        mobile_analysis = analysis_results.get('mobile_performance', {})
        if not mobile_analysis.get('viewport_meta', False):
            issues.append({
                'type': 'mobile',
                'severity': 'high',
                'issue': 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ viewport meta tag',
                'recommendation': 'Ø¥Ø¶Ø§ÙØ© viewport meta tag Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¬ÙˆØ§Ù„'
            })
        
        return issues
    
    def generate_recommendations(self, analysis_results):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        recommendations.append({
            'category': 'performance',
            'priority': 'high',
            'title': 'ØªØ­Ø³ÙŠÙ† Ø³Ø±Ø¹Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„',
            'description': 'ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ± ÙˆØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„ÙØ§Øª',
            'expected_impact': 'ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ø±ØªØ¯Ø§Ø¯',
            'implementation_difficulty': 'medium'
        })
        
        # ØªÙˆØµÙŠØ§Øª SEO
        recommendations.append({
            'category': 'seo',
            'priority': 'high',
            'title': 'ØªØ­Ø³ÙŠÙ† Ø¹Ù†Ø§ØµØ± SEO',
            'description': 'ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„Ø£ÙˆØµØ§Ù ÙˆØ¥Ø¶Ø§ÙØ© ALT text',
            'expected_impact': 'ØªØ­Ø³ÙŠÙ† ØªØ±ØªÙŠØ¨ Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø«',
            'implementation_difficulty': 'low'
        })
        
        # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¬ÙˆØ§Ù„
        recommendations.append({
            'category': 'mobile',
            'priority': 'high',
            'title': 'ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¬ÙˆØ§Ù„',
            'description': 'Ø¥Ø¶Ø§ÙØ© viewport meta tag ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ù„Ù„Ù„Ù…Ø³',
            'expected_impact': 'ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ§Ù„',
            'implementation_difficulty': 'medium'
        })
        
        return recommendations
    
    def run_full_analysis(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙ‚Ù†ÙŠ Ù„Ù…ÙˆÙ‚Ø¹ dnmeg.com...")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
        homepage_performance = self.measure_page_load_time(self.base_url)
        self.results['page_load_times']['homepage'] = homepage_performance
        
        # ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        print("ğŸ“¦ ØªØ­Ù„ÙŠÙ„ ØµÙØ­Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª...")
        product_urls = [
            f"{self.base_url}/products/tee-v1",
            f"{self.base_url}/products/tee-v2",
            f"{self.base_url}/products/jeans-1-9"
        ]
        
        for url in product_urls:
            try:
                performance = self.measure_page_load_time(url)
                page_name = url.split('/')[-1]
                self.results['page_load_times'][page_name] = performance
                print(f"âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ {page_name}")
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {url}: {e}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
        try:
            response = self.session.get(self.base_url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±
            self.results['image_analysis'] = self.analyze_images(soup, self.base_url)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ§Ù„
            self.results['mobile_performance'] = self.test_mobile_performance(self.base_url)
            
            # ØªØ­Ù„ÙŠÙ„ SEO
            self.results['seo_analysis'] = self.analyze_seo(soup, self.base_url)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ: {e}")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        self.results['technical_issues'] = self.identify_technical_issues(self.results)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        self.results['recommendations'] = self.generate_recommendations(self.results)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        with open('dnmeg_performance_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø¨Ù†Ø¬Ø§Ø­!")
        print("ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ dnmeg_performance_analysis.json")
        
        return self.results
    
    def print_summary(self):
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        print("\n" + "="*60)
        print("ğŸ“Š Ù…Ù„Ø®Øµ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙ‚Ù†ÙŠ:")
        print("="*60)
        
        # Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªØ­Ù…ÙŠÙ„
        print(f"âš¡ ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {self.results['page_load_times'].get('homepage', {}).get('load_time', 'N/A')} Ø«Ø§Ù†ÙŠØ©")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        img_analysis = self.results.get('image_analysis', {})
        print(f"ğŸ–¼ï¸ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙˆØ±: {img_analysis.get('total_images', 0)}")
        print(f"ğŸ“¸ ØµÙˆØ± Ø¨Ø¯ÙˆÙ† ALT: {img_analysis.get('missing_alt', 0)}")
        print(f"ğŸ“ ØµÙˆØ± ÙƒØ¨ÙŠØ±Ø©: {img_analysis.get('large_images', 0)}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ§Ù„
        mobile = self.results.get('mobile_performance', {})
        print(f"ğŸ“± ÙˆÙ‚Øª ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¬ÙˆØ§Ù„: {mobile.get('mobile_load_time', 'N/A')} Ø«Ø§Ù†ÙŠØ©")
        print(f"ğŸ“± viewport meta: {'âœ…' if mobile.get('viewport_meta') else 'âŒ'}")
        
        # ØªØ­Ù„ÙŠÙ„ SEO
        seo = self.results.get('seo_analysis', {})
        print(f"ğŸ” Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©: {'âœ…' if seo.get('title', {}).get('optimal') else 'âŒ'}")
        print(f"ğŸ“ ÙˆØµÙ Ø§Ù„ØµÙØ­Ø©: {'âœ…' if seo.get('meta_description', {}).get('optimal') else 'âŒ'}")
        
        # Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        issues = self.results.get('technical_issues', [])
        print(f"âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„: {len(issues)}")
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = self.results.get('recommendations', [])
        print(f"ğŸ’¡ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª: {len(recommendations)}")
        
        print("="*60)

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = PerformanceAnalyzer()
    results = analyzer.run_full_analysis()
    analyzer.print_summary()

if __name__ == "__main__":
    main()
